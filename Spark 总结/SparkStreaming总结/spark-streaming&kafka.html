<!DOCTYPE html>
<html>
<head>
<title>spark-streaming</title>
<meta http-equiv="Content-Type" content="text/html; charset=gb2312" />
<style type="text/css">
/* GitHub stylesheet for MarkdownPad (http://markdownpad.com) */
/* Author: Nicolas Hery - http://nicolashery.com */
/* Version: b13fe65ca28d2e568c6ed5d7f06581183df8f2ff */
/* Source: https://github.com/nicolahery/markdownpad-github */

/* RESET
=============================================================================*/

html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p, blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em, img, ins, kbd, q, s, samp, small, strike, strong, sub, sup, tt, var, b, u, i, center, dl, dt, dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot, thead, tr, th, td, article, aside, canvas, details, embed, figure, figcaption, footer, header, hgroup, menu, nav, output, ruby, section, summary, time, mark, audio, video {
  margin: 0;
  padding: 0;
  border: 0;
}

/* BODY
=============================================================================*/

body {
  font-family: Helvetica, arial, freesans, clean, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  color: #333;
  background-color: #fff;
  padding: 20px;
  max-width: 960px;
  margin: 0 auto;
}

body>*:first-child {
  margin-top: 0 !important;
}

body>*:last-child {
  margin-bottom: 0 !important;
}

/* BLOCKS
=============================================================================*/

p, blockquote, ul, ol, dl, table, pre {
  margin: 15px 0;
}

/* HEADERS
=============================================================================*/

h1, h2, h3, h4, h5, h6 {
  margin: 20px 0 10px;
  padding: 0;
  font-weight: bold;
  -webkit-font-smoothing: antialiased;
}

h1 tt, h1 code, h2 tt, h2 code, h3 tt, h3 code, h4 tt, h4 code, h5 tt, h5 code, h6 tt, h6 code {
  font-size: inherit;
}

h1 {
  font-size: 28px;
  color: #000;
}

h2 {
  font-size: 24px;
  border-bottom: 1px solid #ccc;
  color: #000;
}

h3 {
  font-size: 18px;
}

h4 {
  font-size: 16px;
}

h5 {
  font-size: 14px;
}

h6 {
  color: #777;
  font-size: 14px;
}

body>h2:first-child, body>h1:first-child, body>h1:first-child+h2, body>h3:first-child, body>h4:first-child, body>h5:first-child, body>h6:first-child {
  margin-top: 0;
  padding-top: 0;
}

a:first-child h1, a:first-child h2, a:first-child h3, a:first-child h4, a:first-child h5, a:first-child h6 {
  margin-top: 0;
  padding-top: 0;
}

h1+p, h2+p, h3+p, h4+p, h5+p, h6+p {
  margin-top: 10px;
}

/* LINKS
=============================================================================*/

a {
  color: #4183C4;
  text-decoration: none;
}

a:hover {
  text-decoration: underline;
}

/* LISTS
=============================================================================*/

ul, ol {
  padding-left: 30px;
}

ul li > :first-child, 
ol li > :first-child, 
ul li ul:first-of-type, 
ol li ol:first-of-type, 
ul li ol:first-of-type, 
ol li ul:first-of-type {
  margin-top: 0px;
}

ul ul, ul ol, ol ol, ol ul {
  margin-bottom: 0;
}

dl {
  padding: 0;
}

dl dt {
  font-size: 14px;
  font-weight: bold;
  font-style: italic;
  padding: 0;
  margin: 15px 0 5px;
}

dl dt:first-child {
  padding: 0;
}

dl dt>:first-child {
  margin-top: 0px;
}

dl dt>:last-child {
  margin-bottom: 0px;
}

dl dd {
  margin: 0 0 15px;
  padding: 0 15px;
}

dl dd>:first-child {
  margin-top: 0px;
}

dl dd>:last-child {
  margin-bottom: 0px;
}

/* CODE
=============================================================================*/

pre, code, tt {
  font-size: 12px;
  font-family: Consolas, "Liberation Mono", Courier, monospace;
}

code, tt {
  margin: 0 0px;
  padding: 0px 0px;
  white-space: nowrap;
  border: 1px solid #eaeaea;
  background-color: #f8f8f8;
  border-radius: 3px;
}

pre>code {
  margin: 0;
  padding: 0;
  white-space: pre;
  border: none;
  background: transparent;
}

pre {
  background-color: #f8f8f8;
  border: 1px solid #ccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px;
}

pre code, pre tt {
  background-color: transparent;
  border: none;
}

kbd {
    -moz-border-bottom-colors: none;
    -moz-border-left-colors: none;
    -moz-border-right-colors: none;
    -moz-border-top-colors: none;
    background-color: #DDDDDD;
    background-image: linear-gradient(#F1F1F1, #DDDDDD);
    background-repeat: repeat-x;
    border-color: #DDDDDD #CCCCCC #CCCCCC #DDDDDD;
    border-image: none;
    border-radius: 2px 2px 2px 2px;
    border-style: solid;
    border-width: 1px;
    font-family: "Helvetica Neue",Helvetica,Arial,sans-serif;
    line-height: 10px;
    padding: 1px 4px;
}

/* QUOTES
=============================================================================*/

blockquote {
  border-left: 4px solid #DDD;
  padding: 0 15px;
  color: #777;
}

blockquote>:first-child {
  margin-top: 0px;
}

blockquote>:last-child {
  margin-bottom: 0px;
}

/* HORIZONTAL RULES
=============================================================================*/

hr {
  clear: both;
  margin: 15px 0;
  height: 0px;
  overflow: hidden;
  border: none;
  background: transparent;
  border-bottom: 4px solid #ddd;
  padding: 0;
}

/* TABLES
=============================================================================*/

table th {
  font-weight: bold;
}

table th, table td {
  border: 1px solid #ccc;
  padding: 6px 13px;
}

table tr {
  border-top: 1px solid #ccc;
  background-color: #fff;
}

table tr:nth-child(2n) {
  background-color: #f8f8f8;
}

/* IMAGES
=============================================================================*/

img {
  max-width: 100%
}
</style>
</head>
<body>
<h1>Spark-Streaming&Kafka</h1>
<h3><div align="right">王炎</div></h3>
<h3>1. 概述:</h3>
<p>kafka是一种高吞吐量的分布式订阅消息系统，它可以处理消费者规模的网站中的所有动作流数据。这种动作(网页浏览器，搜索和其他用户的行动)是在现代网络上的许多社会功能的一个关键因素。这些数据通常是由于吞吐量的要求而通过处理日志和日志聚合来解决。</p>
<p>kafka是一种高吞吐量的分布式订阅消息系统,有如下特性：<p>
<p>其内部工作方式如下：</p>
<p>1.通过O(1)的磁盘数据结构提供消息的持久化，这种数据结构对于即使数以TB的消息存储也能够保持长时间的稳定性能。</p>
<p>2.高吞吐量：即使是非常普通的硬件kafka也可=以支持每秒数十万的消息。</p>
<p>3.支持通过kafka服务器和消费机集群来分区消息。</p>
<p>4.支持Hadoop并行数据加载。</p>

<p>在官网上，kafka的定义是：Kafka是一个分布式的、可分区的、可复制的消息系统。</p>
<p>我们来了解一下kafka中几个基本的消息系统术语：</p>
<p>&nbsp;&nbsp;&nbsp;Topic: kafka将消息以topic为单位进行归纳。</p>
<p>&nbsp;&nbsp;&nbsp;Consumer:将预订topics并消费消息的程序称为consumer</p>
<p>&nbsp;&nbsp;&nbsp;Broker: Kafka以集群的方式运行，可以由一个或多个服务组成，每一个服务叫做一个broker.</p>
<p>producers通过网络将消息发送到kafka集群,集群向消费者提供消息，如下图所示：</p>
<p><img src="img/producer_consumer.png"/></p>
<h3>2. 搭建Kafka的环境</h3>
<p> Step1：下载Kafka</p>
<pre><code>
    到官网上下载最新版kafka_2.10-0.8.2.1.tgz
    然后解压 sudo tar -zxvf kafka_2.10-0.8.2.1.tgz
    进入到kafka cd kafka_2.10-0.8.2.1
</code></pre>
<p> Step2：启动服务</p>
<pre><code>
    kafka用到了Zookeeper,所以首先启动Zookeeper的服务
    bin/zookeeper-server-start.sh config/zookeeper.properties
    启动kafka
    bin/kafka-server-start.sh config/server.properties
</code></pre>
<p> Step3：创建topic</p>
<pre><code>
    创建一个叫做'test'的topic,它只有一个分区，一个副本
    bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test
    通过list命令查看创建的topic
    bin/kafka-topics.sh --list --zookeeper localhost:2181
</code></pre>
<p>除了可以手动创建topic，还可以配置broker让它自动创建topic</p>
<p> Step4:发送消息</p>
<pre><code>
    Kafka使用一个简单的命令行producer，从文件中或者是从标准输入流中读取消息并发送到服务器端。默认的每条命令将发送一条消息
    运行producer并在控制台输出一些消息，这些消息将被发送到服务器端
    bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test
</code></pre>
<p>ctrl+c退出发送</p>
<p>Step5:启动consumer</p>
<pre><code>
    Kafka也有一个命令行consumer可以读取消息并输出到标准到标准输出：
    bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test --from-beginning
</code></pre>
<p>你在一个终端中运行consumer命令行,另一个终端中运行producer命令行，就可以在一个终端输入消息，另一个终端读取消息。</p>
<h3>3. 读取Kafka数据方法</h3>
<p>在org.apache.spark.streaming.kafka的包中有一个KafkaUtils能够帮助我们读取kafka的数据</p>
<p>在其中提供了两种读取kafka数据的方法</p>
<p>第一种方法(Receiver-based Approach)</p>
<p>在KafkaUtils中提供了createStream的方法,这个方法返回的是一个ReceiverInputStream的输入流，在kafka brokers上接收信息（需要一个接收器receiver）</p>
<pre><code>
    这个方法有5个参数：
    ssc: StreamingContext 一个StreamingContext的对象
    zkQuorum: String Zookeeper服务的主机名加端口号（如：localhost:2181）
    groupId:String 这个consumer组的编号
    topics: Map[String, Int] 指定topic名称和线程数目的一个Map集合
    storageLevel: StorageLevel 指定用于存储接收对象的存储级,默认为： StorageLevel.MEMORY_AND_DISK_SER_2
</code></pre>
<p>第二种方法（No Receiver）</p>
<p>在KafkaUtils中提供了createDirectStream的方法，这个方法返回的是一个InputDStream的输入流,直接从kafka brokers上之间接收消息，无需任何的接收器（直接查询kafka）。能够保证精确的接收到kafka的每一条消息。</p>
<pre><code>
    这个方法有3个参数：
    ssc: StreamingContext 一个StreamingContext的对象
    kafkaParams：Map[String, String] Kafka的配置参数(要求“metadata.broker.list” or "bootstrap.servers"配置kafka brokers(不需要设置zookeeper服务);"auto.offset.reset"可以设置为 "largest" or "smallest"(默认是"largest"))
    topics: Set[String])：topic的一个集合。
</code></pre>
<h3>4. 代码示例</h3>
<p>任务目的：给kafka发送信息，然后接受kafka的数据，统计接受到的单词数量</p>
<p>所需要的jar包支持：</p>
<p><img src="img/jar.png"/></p>
<p>发送消息到kafka </p>
<pre><code>
 object KafkaWordCountProducer{
   def main(args: Array[String]) {
    val brokers="localhost:9092"
    val topic="test"
    val messagesPerSec=1
    val wordsPerMessage=3

    // Zookeeper connection properties
    val proprs = new util.HashMap[String,Object]()
    proprs.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,brokers)
    proprs.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,"org.apache.kafka.common.serialization.StringSerializer")
    proprs.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,"org.apache.kafka.common.serialization.StringSerializer")

    val producer=new KafkaProducer[String,String](proprs)
    //send to message
    while(true){
      (1 to messagesPerSec.toInt).foreach{ messageNum =>
         val str = (1 to wordsPerMessage.toInt).map(x => scala.util.Random.nextInt(10).toString).mkString(" ")
         val message = new ProducerRecord[String,String](topic,null,str)
        producer.send(message)
      }
      Thread.sleep(1000)
    }
  }
}
</code></pre>
<p>首先需要配置Zookeeper的连接属性，然后new出一个kafka数据的生产对象KafkaProducer和需要发送的消息对象ProducerRecord</p>
<p>我们可以查看到发送到kafka中的数据</p>
<pre><code>
bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test --from-beginning
</code></pre>
<p>部分运行结果：</p>
<pre><code>
8 7 0 
9 4 2
7 7 9
6 7 5
5 2 5
</code></pre>
<p>接受消息并统计单词数目 </p>
<p>第一种（通过调用createStream方法接收数据）</p>
<pre><code>
 object SparkStreamKafkaDemo1 {
  def main(args: Array[String]) {
    val zkQuorum="localhost:2181"
    val groupId="1"
    val topics="test"
    val numThreads=2
    val conf = new SparkConf().setAppName("sparkstreamkafkademo1").setMaster("local[*]")
    val ssc = new StreamingContext(conf, Seconds(2))
    ssc.checkpoint("checkpoint")

    val topicMap = topics.split(" ").map((_, numThreads.toInt)).toMap
    val messages = KafkaUtils.createStream(ssc,zkQuorum,groupId,topicMap)
    val lines=messages.map(_._2)
    val words = lines.flatMap(_.split(" "))
    val wordCounts = words.map(x => (x, 1L)).reduceByKeyAndWindow(_ + _, _ - _, Minutes(10), Seconds(2), 2)
    wordCounts.print()
    ssc.start()
    ssc.awaitTermination()
  }
}
</code></pre>
<p>第二种（通过调用createDirectStream方法接收数据）</p>
<pre><code>
object SparkStreamKafkaDemo2 {
  def main(args: Array[String]) {
    val brokers="localhost:9092"
    val topics="test"
    val conf = new SparkConf().setAppName("sparkstreamkafkademo2").setMaster("local[*]")
    val ssc = new StreamingContext(conf,Seconds(2))

    // Create direct kafka stream with brokers and topics
    val topicsSet=topics.split(",").toSet
    val kafkaParams=Map[String,String]("metadata.broker.list"->brokers)
    val messages=KafkaUtils.createDirectStream[String,String,StringDecoder,StringDecoder](ssc,kafkaParams,topicsSet)

    // Get the lines, split them into words, count the words and print
    val lines = messages.map(_._2)
    val words = lines.flatMap(_.split(" "))
    val wordCounts = words.map(x=>(x,1L)).reduceByKey(_+_)
    wordCounts.print()

    // Start the computation
    ssc.start()
    ssc.awaitTermination()
  }
}
</code></pre>
</body>
</html>
<!-- This document was created with MarkdownPad, the Markdown editor for Windows (http://markdownpad.com) -->
