SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/hadoop/tools/spark1.4/lib/spark-assembly-1.4.0-hadoop2.6.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/hadoop/tools/spark1.4/lib/spark-examples-1.4.0-hadoop2.6.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/04/14 21:21:31 INFO SparkContext: Running Spark version 1.4.0
16/04/14 21:21:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/04/14 21:21:38 INFO SecurityManager: Changing view acls to: hadoop
16/04/14 21:21:38 INFO SecurityManager: Changing modify acls to: hadoop
16/04/14 21:21:38 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hadoop); users with modify permissions: Set(hadoop)
16/04/14 21:21:41 INFO Slf4jLogger: Slf4jLogger started
16/04/14 21:21:41 INFO Remoting: Starting remoting
16/04/14 21:21:43 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.25.128:46271]
16/04/14 21:21:43 INFO Utils: Successfully started service 'sparkDriver' on port 46271.
16/04/14 21:21:43 INFO SparkEnv: Registering MapOutputTracker
16/04/14 21:21:44 INFO SparkEnv: Registering BlockManagerMaster
16/04/14 21:21:44 INFO DiskBlockManager: Created local directory at /tmp/spark-ade768e5-1814-4c89-9a68-e6cdd774728c/blockmgr-4f31e42c-16f5-4306-bd25-5a0437ad3709
16/04/14 21:21:44 INFO MemoryStore: MemoryStore started with capacity 415.5 MB
16/04/14 21:21:45 INFO HttpFileServer: HTTP File server directory is /tmp/spark-ade768e5-1814-4c89-9a68-e6cdd774728c/httpd-a2743a77-4bd5-49ae-82b6-168d55dfdb43
16/04/14 21:21:45 INFO HttpServer: Starting HTTP Server
16/04/14 21:21:45 INFO Utils: Successfully started service 'HTTP file server' on port 39990.
16/04/14 21:21:46 INFO SparkEnv: Registering OutputCommitCoordinator
16/04/14 21:21:47 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/04/14 21:21:47 INFO SparkUI: Started SparkUI at http://192.168.25.128:4040
16/04/14 21:21:48 INFO Executor: Starting executor ID driver on host localhost
16/04/14 21:21:49 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39076.
16/04/14 21:21:49 INFO NettyBlockTransferService: Server created on 39076
16/04/14 21:21:49 INFO BlockManagerMaster: Trying to register BlockManager
16/04/14 21:21:49 INFO BlockManagerMasterEndpoint: Registering block manager localhost:39076 with 415.5 MB RAM, BlockManagerId(driver, localhost, 39076)
16/04/14 21:21:49 INFO BlockManagerMaster: Registered BlockManager
16/04/14 21:22:06 INFO SparkContext: Added JAR /home/hadoop/tools/jars/1.jar at http://192.168.25.128:39990/jars/1.jar with timestamp 1460640126321
16/04/14 21:22:08 INFO MemoryStore: ensureFreeSpace(130448) called with curMem=0, maxMem=435714785
16/04/14 21:22:08 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 127.4 KB, free 415.4 MB)
16/04/14 21:22:08 INFO MemoryStore: ensureFreeSpace(14257) called with curMem=130448, maxMem=435714785
16/04/14 21:22:08 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.9 KB, free 415.4 MB)
16/04/14 21:22:08 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:39076 (size: 13.9 KB, free: 415.5 MB)
16/04/14 21:22:08 INFO SparkContext: Created broadcast 0 from textFile at Ex0Wordcount.java:56
16/04/14 21:22:11 INFO FileInputFormat: Total input paths to process : 1
16/04/14 21:22:11 INFO SparkContext: Starting job: collect at Ex0Wordcount.java:104
16/04/14 21:22:12 INFO DAGScheduler: Registering RDD 3 (mapToPair at Ex0Wordcount.java:75)
16/04/14 21:22:12 INFO DAGScheduler: Got job 0 (collect at Ex0Wordcount.java:104) with 1 output partitions (allowLocal=false)
16/04/14 21:22:12 INFO DAGScheduler: Final stage: ResultStage 1(collect at Ex0Wordcount.java:104)
16/04/14 21:22:12 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
16/04/14 21:22:12 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
16/04/14 21:22:12 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at Ex0Wordcount.java:75), which has no missing parents
16/04/14 21:22:12 INFO MemoryStore: ensureFreeSpace(4800) called with curMem=144705, maxMem=435714785
16/04/14 21:22:12 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 4.7 KB, free 415.4 MB)
16/04/14 21:22:12 INFO MemoryStore: ensureFreeSpace(2701) called with curMem=149505, maxMem=435714785
16/04/14 21:22:12 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.6 KB, free 415.4 MB)
16/04/14 21:22:12 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:39076 (size: 2.6 KB, free: 415.5 MB)
16/04/14 21:22:12 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:874
16/04/14 21:22:12 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at Ex0Wordcount.java:75)
16/04/14 21:22:12 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/04/14 21:22:13 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1468 bytes)
16/04/14 21:22:13 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/04/14 21:22:13 INFO Executor: Fetching http://192.168.25.128:39990/jars/1.jar with timestamp 1460640126321
16/04/14 21:22:13 INFO Utils: Fetching http://192.168.25.128:39990/jars/1.jar to /tmp/spark-ade768e5-1814-4c89-9a68-e6cdd774728c/userFiles-29a180ca-ea84-49a1-8896-71879ee5ba6d/fetchFileTemp7897058475024632863.tmp
16/04/14 21:22:24 INFO Executor: Adding file:/tmp/spark-ade768e5-1814-4c89-9a68-e6cdd774728c/userFiles-29a180ca-ea84-49a1-8896-71879ee5ba6d/1.jar to class loader
16/04/14 21:22:24 INFO HadoopRDD: Input split: file:/home/hadoop/workspace1/SparkTest1/data/wordcount.txt:0+4921
16/04/14 21:22:24 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
16/04/14 21:22:24 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
16/04/14 21:22:24 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
16/04/14 21:22:24 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
16/04/14 21:22:24 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
16/04/14 21:22:26 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2001 bytes result sent to driver
16/04/14 21:22:26 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 13433 ms on localhost (1/1)
16/04/14 21:22:26 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/04/14 21:22:26 INFO DAGScheduler: ShuffleMapStage 0 (mapToPair at Ex0Wordcount.java:75) finished in 13.591 s
16/04/14 21:22:26 INFO DAGScheduler: looking for newly runnable stages
16/04/14 21:22:26 INFO DAGScheduler: running: Set()
16/04/14 21:22:26 INFO DAGScheduler: waiting: Set(ResultStage 1)
16/04/14 21:22:26 INFO DAGScheduler: failed: Set()
16/04/14 21:22:26 INFO DAGScheduler: Missing parents for ResultStage 1: List()
16/04/14 21:22:26 INFO DAGScheduler: Submitting ResultStage 1 (ShuffledRDD[4] at reduceByKey at Ex0Wordcount.java:86), which is now runnable
16/04/14 21:22:26 INFO MemoryStore: ensureFreeSpace(2464) called with curMem=152206, maxMem=435714785
16/04/14 21:22:26 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 2.4 KB, free 415.4 MB)
16/04/14 21:22:26 INFO MemoryStore: ensureFreeSpace(1496) called with curMem=154670, maxMem=435714785
16/04/14 21:22:26 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 1496.0 B, free 415.4 MB)
16/04/14 21:22:26 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:39076 (size: 1496.0 B, free: 415.5 MB)
16/04/14 21:22:26 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:874
16/04/14 21:22:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (ShuffledRDD[4] at reduceByKey at Ex0Wordcount.java:86)
16/04/14 21:22:26 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
16/04/14 21:22:26 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, PROCESS_LOCAL, 1213 bytes)
16/04/14 21:22:26 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
16/04/14 21:22:27 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
16/04/14 21:22:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 83 ms
16/04/14 21:22:27 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 9120 bytes result sent to driver
16/04/14 21:22:27 INFO DAGScheduler: ResultStage 1 (collect at Ex0Wordcount.java:104) finished in 0.720 s
16/04/14 21:22:27 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 715 ms on localhost (1/1)
16/04/14 21:22:27 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
16/04/14 21:22:27 INFO DAGScheduler: Job 0 finished: collect at Ex0Wordcount.java:104, took 15.962110 s
be==8
may==8
or==11
of==25
for==10
rules==5
count==11
words==21
in==11
word==24
that==5
The==5
such==7
the==38
as==9
is==19
text==8
on==7
counting==6
also==5
can==5
by==5
a==28
to==18
length==5
and==23
16/04/14 21:22:27 INFO SparkContext: Invoking stop() from shutdown hook
16/04/14 21:22:28 INFO SparkUI: Stopped Spark web UI at http://192.168.25.128:4040
16/04/14 21:22:28 INFO DAGScheduler: Stopping DAGScheduler
16/04/14 21:22:28 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/04/14 21:22:28 INFO Utils: path = /tmp/spark-ade768e5-1814-4c89-9a68-e6cdd774728c/blockmgr-4f31e42c-16f5-4306-bd25-5a0437ad3709, already present as root for deletion.
16/04/14 21:22:28 INFO MemoryStore: MemoryStore cleared
16/04/14 21:22:28 INFO BlockManager: BlockManager stopped
16/04/14 21:22:28 INFO BlockManagerMaster: BlockManagerMaster stopped
16/04/14 21:22:28 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/04/14 21:22:28 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
16/04/14 21:22:28 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
16/04/14 21:22:28 INFO SparkContext: Successfully stopped SparkContext
16/04/14 21:22:28 INFO Utils: Shutdown hook called
16/04/14 21:22:28 INFO Utils: Deleting directory /tmp/spark-ade768e5-1814-4c89-9a68-e6cdd774728c
