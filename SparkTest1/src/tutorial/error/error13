SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/hadoop/tools/spark1.4/lib/spark-assembly-1.4.0-hadoop2.6.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/hadoop/tools/spark1.4/lib/spark-examples-1.4.0-hadoop2.6.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/04/14 20:06:44 INFO SparkContext: Running Spark version 1.4.0
16/04/14 20:06:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/04/14 20:06:47 INFO SecurityManager: Changing view acls to: hadoop
16/04/14 20:06:47 INFO SecurityManager: Changing modify acls to: hadoop
16/04/14 20:06:47 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hadoop); users with modify permissions: Set(hadoop)
16/04/14 20:06:54 INFO Slf4jLogger: Slf4jLogger started
16/04/14 20:06:54 INFO Remoting: Starting remoting
16/04/14 20:06:55 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.25.128:37078]
16/04/14 20:06:55 INFO Utils: Successfully started service 'sparkDriver' on port 37078.
16/04/14 20:06:55 INFO SparkEnv: Registering MapOutputTracker
16/04/14 20:06:55 INFO SparkEnv: Registering BlockManagerMaster
16/04/14 20:06:55 INFO DiskBlockManager: Created local directory at /tmp/spark-8289d649-d8c8-4d28-b29f-cc19603667d0/blockmgr-921d268a-9ce7-4a4c-9b29-38347f29c717
16/04/14 20:06:55 INFO MemoryStore: MemoryStore started with capacity 731.9 MB
16/04/14 20:06:56 INFO HttpFileServer: HTTP File server directory is /tmp/spark-8289d649-d8c8-4d28-b29f-cc19603667d0/httpd-5526da65-9ef0-4c8d-bfbc-7fc1081b3e04
16/04/14 20:06:56 INFO HttpServer: Starting HTTP Server
16/04/14 20:06:56 INFO Utils: Successfully started service 'HTTP file server' on port 46390.
16/04/14 20:06:56 INFO SparkEnv: Registering OutputCommitCoordinator
16/04/14 20:07:00 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/04/14 20:07:00 INFO SparkUI: Started SparkUI at http://192.168.25.128:4040
16/04/14 20:07:00 INFO AppClient$ClientActor: Connecting to master akka.tcp://sparkMaster@master:7077/user/Master...
16/04/14 20:07:02 INFO SparkDeploySchedulerBackend: Connected to Spark cluster with app ID app-20160414200702-0004
16/04/14 20:07:03 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39467.
16/04/14 20:07:03 INFO NettyBlockTransferService: Server created on 39467
16/04/14 20:07:03 INFO BlockManagerMaster: Trying to register BlockManager
16/04/14 20:07:03 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.25.128:39467 with 731.9 MB RAM, BlockManagerId(driver, 192.168.25.128, 39467)
16/04/14 20:07:03 INFO BlockManagerMaster: Registered BlockManager
16/04/14 20:07:04 INFO SparkDeploySchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
16/04/14 20:07:28 INFO SparkContext: Added JAR /home/hadoop/tools/jars/1.jar at http://192.168.25.128:46390/jars/1.jar with timestamp 1460635648481
16/04/14 20:07:28 INFO SparkContext: Running Spark version 1.4.0
16/04/14 20:07:28 INFO SecurityManager: Changing view acls to: hadoop
16/04/14 20:07:28 INFO SecurityManager: Changing modify acls to: hadoop
16/04/14 20:07:28 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hadoop); users with modify permissions: Set(hadoop)
16/04/14 20:07:28 INFO Slf4jLogger: Slf4jLogger started
16/04/14 20:07:28 INFO Remoting: Starting remoting
16/04/14 20:07:28 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.25.128:42261]
16/04/14 20:07:28 INFO Utils: Successfully started service 'sparkDriver' on port 42261.
16/04/14 20:07:29 INFO SparkEnv: Registering MapOutputTracker
16/04/14 20:07:29 INFO SparkEnv: Registering BlockManagerMaster
16/04/14 20:07:29 INFO DiskBlockManager: Created local directory at /tmp/spark-8289d649-d8c8-4d28-b29f-cc19603667d0/blockmgr-4fb86bde-526b-4a7b-bcf5-490b2d25d5cb
16/04/14 20:07:29 INFO MemoryStore: MemoryStore started with capacity 731.9 MB
16/04/14 20:07:29 INFO HttpFileServer: HTTP File server directory is /tmp/spark-8289d649-d8c8-4d28-b29f-cc19603667d0/httpd-095f3f60-8023-48e2-ac67-6e1ba37e2414
16/04/14 20:07:29 INFO HttpServer: Starting HTTP Server
16/04/14 20:07:29 INFO Utils: Successfully started service 'HTTP file server' on port 44276.
16/04/14 20:07:29 INFO SparkEnv: Registering OutputCommitCoordinator
16/04/14 20:07:29 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
16/04/14 20:07:29 INFO Utils: Successfully started service 'SparkUI' on port 4041.
16/04/14 20:07:29 INFO SparkUI: Started SparkUI at http://192.168.25.128:4041
16/04/14 20:07:30 INFO AppClient$ClientActor: Connecting to master akka.tcp://sparkMaster@master:7077/user/Master...
16/04/14 20:07:30 INFO SparkDeploySchedulerBackend: Connected to Spark cluster with app ID app-20160414200730-0005
16/04/14 20:07:30 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40407.
16/04/14 20:07:30 INFO NettyBlockTransferService: Server created on 40407
16/04/14 20:07:30 INFO BlockManagerMaster: Trying to register BlockManager
16/04/14 20:07:30 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.25.128:40407 with 731.9 MB RAM, BlockManagerId(driver, 192.168.25.128, 40407)
16/04/14 20:07:30 INFO BlockManagerMaster: Registered BlockManager
16/04/14 20:07:30 INFO AppClient$ClientActor: Executor added: app-20160414200730-0005/0 on worker-20160414190248-192.168.25.128-45954 (192.168.25.128:45954) with 1 cores
16/04/14 20:07:30 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160414200730-0005/0 on hostPort 192.168.25.128:45954 with 1 cores, 512.0 MB RAM
16/04/14 20:07:30 INFO SparkDeploySchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
16/04/14 20:07:30 WARN SparkContext: Multiple running SparkContexts detected in the same JVM!
org.apache.spark.SparkException: Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
tutorial.Test_01.main(Test_01.java:16)
	at org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$1.apply(SparkContext.scala:2083)
	at org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$1.apply(SparkContext.scala:2065)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2065)
	at org.apache.spark.SparkContext$.setActiveContext(SparkContext.scala:2151)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:2023)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at tutorial.Ex0Wordcount.loadData(Ex0Wordcount.java:47)
	at tutorial.Ex0Wordcount.wordcount(Ex0Wordcount.java:64)
	at tutorial.Ex0Wordcount.filterOnWordcount(Ex0Wordcount.java:91)
	at tutorial.Test_01.main(Test_01.java:20)
16/04/14 20:07:30 INFO AppClient$ClientActor: Executor updated: app-20160414200730-0005/0 is now RUNNING
16/04/14 20:07:30 INFO AppClient$ClientActor: Executor updated: app-20160414200730-0005/0 is now LOADING
16/04/14 20:07:35 INFO MemoryStore: ensureFreeSpace(130448) called with curMem=0, maxMem=767420006
16/04/14 20:07:35 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 127.4 KB, free 731.7 MB)
16/04/14 20:07:36 INFO MemoryStore: ensureFreeSpace(14257) called with curMem=130448, maxMem=767420006
16/04/14 20:07:36 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.9 KB, free 731.7 MB)
16/04/14 20:07:36 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.25.128:40407 (size: 13.9 KB, free: 731.9 MB)
16/04/14 20:07:36 INFO SparkContext: Created broadcast 0 from textFile at Ex0Wordcount.java:50
16/04/14 20:07:37 INFO FileInputFormat: Total input paths to process : 1
16/04/14 20:07:38 INFO SparkContext: Starting job: collect at Ex0Wordcount.java:93
16/04/14 20:07:39 INFO DAGScheduler: Registering RDD 3 (mapToPair at Ex0Wordcount.java:67)
16/04/14 20:07:39 INFO DAGScheduler: Got job 0 (collect at Ex0Wordcount.java:93) with 2 output partitions (allowLocal=false)
16/04/14 20:07:39 INFO DAGScheduler: Final stage: ResultStage 1(collect at Ex0Wordcount.java:93)
16/04/14 20:07:39 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
16/04/14 20:07:39 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
16/04/14 20:07:39 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at Ex0Wordcount.java:67), which has no missing parents
16/04/14 20:07:39 INFO MemoryStore: ensureFreeSpace(4800) called with curMem=144705, maxMem=767420006
16/04/14 20:07:39 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 4.7 KB, free 731.7 MB)
16/04/14 20:07:39 INFO MemoryStore: ensureFreeSpace(2699) called with curMem=149505, maxMem=767420006
16/04/14 20:07:39 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.6 KB, free 731.7 MB)
16/04/14 20:07:39 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.25.128:40407 (size: 2.6 KB, free: 731.9 MB)
16/04/14 20:07:39 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:874
16/04/14 20:07:39 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at Ex0Wordcount.java:67)
16/04/14 20:07:39 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/04/14 20:07:54 INFO SparkDeploySchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@192.168.25.128:38900/user/Executor#-1883922649]) with ID 0
16/04/14 20:07:55 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
16/04/14 20:07:55 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.25.128, PROCESS_LOCAL, 1420 bytes)
16/04/14 20:07:59 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.25.128:44898 with 267.3 MB RAM, BlockManagerId(0, 192.168.25.128, 44898)
16/04/14 20:08:08 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.25.128:44898 (size: 2.6 KB, free: 267.3 MB)
16/04/14 20:08:08 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, 192.168.25.128, PROCESS_LOCAL, 1420 bytes)
16/04/14 20:08:08 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, 192.168.25.128): java.lang.ClassNotFoundException: tutorial.Ex0Wordcount$1
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:66)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1613)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1518)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1774)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)
	at scala.collection.immutable.$colon$colon.readObject(List.scala:362)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1896)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:69)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:95)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:61)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:70)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

16/04/14 20:08:09 INFO TaskSetManager: Starting task 0.1 in stage 0.0 (TID 2, 192.168.25.128, PROCESS_LOCAL, 1420 bytes)
16/04/14 20:08:09 INFO TaskSetManager: Lost task 1.0 in stage 0.0 (TID 1) on executor 192.168.25.128: java.lang.ClassNotFoundException (tutorial.Ex0Wordcount$1) [duplicate 1]
16/04/14 20:08:09 INFO TaskSetManager: Starting task 1.1 in stage 0.0 (TID 3, 192.168.25.128, PROCESS_LOCAL, 1420 bytes)
16/04/14 20:08:09 INFO TaskSetManager: Lost task 1.1 in stage 0.0 (TID 3) on executor 192.168.25.128: java.lang.ClassNotFoundException (tutorial.Ex0Wordcount$1) [duplicate 2]
16/04/14 20:08:09 INFO TaskSetManager: Lost task 0.1 in stage 0.0 (TID 2) on executor 192.168.25.128: java.lang.ClassNotFoundException (tutorial.Ex0Wordcount$1) [duplicate 3]
16/04/14 20:08:09 INFO TaskSetManager: Starting task 0.2 in stage 0.0 (TID 4, 192.168.25.128, PROCESS_LOCAL, 1420 bytes)
16/04/14 20:08:09 INFO TaskSetManager: Starting task 1.2 in stage 0.0 (TID 5, 192.168.25.128, PROCESS_LOCAL, 1420 bytes)
16/04/14 20:08:09 INFO TaskSetManager: Lost task 0.2 in stage 0.0 (TID 4) on executor 192.168.25.128: java.lang.ClassNotFoundException (tutorial.Ex0Wordcount$1) [duplicate 4]
16/04/14 20:08:09 INFO TaskSetManager: Starting task 0.3 in stage 0.0 (TID 6, 192.168.25.128, PROCESS_LOCAL, 1420 bytes)
16/04/14 20:08:09 INFO TaskSetManager: Lost task 1.2 in stage 0.0 (TID 5) on executor 192.168.25.128: java.lang.ClassNotFoundException (tutorial.Ex0Wordcount$1) [duplicate 5]
16/04/14 20:08:09 INFO TaskSetManager: Starting task 1.3 in stage 0.0 (TID 7, 192.168.25.128, PROCESS_LOCAL, 1420 bytes)
16/04/14 20:08:09 INFO TaskSetManager: Lost task 0.3 in stage 0.0 (TID 6) on executor 192.168.25.128: java.lang.ClassNotFoundException (tutorial.Ex0Wordcount$1) [duplicate 6]
16/04/14 20:08:09 ERROR TaskSetManager: Task 0 in stage 0.0 failed 4 times; aborting job
16/04/14 20:08:09 INFO TaskSchedulerImpl: Cancelling stage 0
16/04/14 20:08:09 INFO TaskSchedulerImpl: Stage 0 was cancelled
16/04/14 20:08:10 INFO TaskSetManager: Lost task 1.3 in stage 0.0 (TID 7) on executor 192.168.25.128: java.lang.ClassNotFoundException (tutorial.Ex0Wordcount$1) [duplicate 7]
16/04/14 20:08:10 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/04/14 20:08:10 INFO DAGScheduler: ShuffleMapStage 0 (mapToPair at Ex0Wordcount.java:67) failed in 30.034 s
16/04/14 20:08:10 INFO DAGScheduler: Job 0 failed: collect at Ex0Wordcount.java:93, took 31.548407 s
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, 192.168.25.128): java.lang.ClassNotFoundException: tutorial.Ex0Wordcount$1
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:66)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1613)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1518)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1774)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)
	at scala.collection.immutable.$colon$colon.readObject(List.scala:362)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1896)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:69)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:95)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:61)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:70)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1266)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1257)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1256)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1256)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1411)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
16/04/14 20:08:10 INFO SparkContext: Invoking stop() from shutdown hook
16/04/14 20:08:10 INFO SparkUI: Stopped Spark web UI at http://192.168.25.128:4040
16/04/14 20:08:10 INFO DAGScheduler: Stopping DAGScheduler
16/04/14 20:08:10 INFO SparkDeploySchedulerBackend: Shutting down all executors
16/04/14 20:08:10 INFO SparkDeploySchedulerBackend: Asking each executor to shut down
16/04/14 20:08:10 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/04/14 20:08:10 INFO Utils: path = /tmp/spark-8289d649-d8c8-4d28-b29f-cc19603667d0/blockmgr-921d268a-9ce7-4a4c-9b29-38347f29c717, already present as root for deletion.
16/04/14 20:08:10 INFO MemoryStore: MemoryStore cleared
16/04/14 20:08:10 INFO BlockManager: BlockManager stopped
16/04/14 20:08:10 INFO BlockManagerMaster: BlockManagerMaster stopped
16/04/14 20:08:10 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/04/14 20:08:10 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
16/04/14 20:08:10 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
16/04/14 20:08:11 INFO SparkContext: Successfully stopped SparkContext
16/04/14 20:08:11 INFO SparkContext: Invoking stop() from shutdown hook
16/04/14 20:08:11 INFO SparkUI: Stopped Spark web UI at http://192.168.25.128:4041
16/04/14 20:08:11 INFO DAGScheduler: Stopping DAGScheduler
16/04/14 20:08:11 INFO SparkDeploySchedulerBackend: Shutting down all executors
16/04/14 20:08:11 INFO SparkDeploySchedulerBackend: Asking each executor to shut down
16/04/14 20:08:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/04/14 20:08:11 INFO Utils: path = /tmp/spark-8289d649-d8c8-4d28-b29f-cc19603667d0/blockmgr-4fb86bde-526b-4a7b-bcf5-490b2d25d5cb, already present as root for deletion.
16/04/14 20:08:11 INFO MemoryStore: MemoryStore cleared
16/04/14 20:08:11 INFO BlockManager: BlockManager stopped
16/04/14 20:08:11 INFO BlockManagerMaster: BlockManagerMaster stopped
16/04/14 20:08:11 INFO SparkContext: Successfully stopped SparkContext
16/04/14 20:08:11 INFO Utils: Shutdown hook called
16/04/14 20:08:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/04/14 20:08:11 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
16/04/14 20:08:11 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
16/04/14 20:08:11 INFO Utils: Deleting directory /tmp/spark-8289d649-d8c8-4d28-b29f-cc19603667d0
