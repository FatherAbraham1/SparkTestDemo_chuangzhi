SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/hadoop/tools/spark1.4/lib/spark-assembly-1.4.0-hadoop2.6.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/hadoop/tools/spark1.4/lib/spark-examples-1.4.0-hadoop2.6.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/04/14 21:07:51 INFO SparkContext: Running Spark version 1.4.0
16/04/14 21:07:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/04/14 21:07:52 INFO SecurityManager: Changing view acls to: hadoop
16/04/14 21:07:52 INFO SecurityManager: Changing modify acls to: hadoop
16/04/14 21:07:52 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hadoop); users with modify permissions: Set(hadoop)
16/04/14 21:07:53 INFO Slf4jLogger: Slf4jLogger started
16/04/14 21:07:53 INFO Remoting: Starting remoting
16/04/14 21:07:53 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.25.128:43556]
16/04/14 21:07:54 INFO Utils: Successfully started service 'sparkDriver' on port 43556.
16/04/14 21:07:54 INFO SparkEnv: Registering MapOutputTracker
16/04/14 21:07:54 INFO SparkEnv: Registering BlockManagerMaster
16/04/14 21:07:54 INFO DiskBlockManager: Created local directory at /tmp/spark-f3c3dac6-604d-430e-8cf9-266f0563f7cb/blockmgr-de143055-81e3-4cc8-b71c-f8d7a35046b6
16/04/14 21:07:54 INFO MemoryStore: MemoryStore started with capacity 415.5 MB
16/04/14 21:07:54 INFO HttpFileServer: HTTP File server directory is /tmp/spark-f3c3dac6-604d-430e-8cf9-266f0563f7cb/httpd-198c7ea5-b7fd-420a-ab9b-b43dccdce5a6
16/04/14 21:07:54 INFO HttpServer: Starting HTTP Server
16/04/14 21:07:54 INFO Utils: Successfully started service 'HTTP file server' on port 33204.
16/04/14 21:07:54 INFO SparkEnv: Registering OutputCommitCoordinator
16/04/14 21:07:55 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/04/14 21:07:55 INFO SparkUI: Started SparkUI at http://192.168.25.128:4040
16/04/14 21:07:55 INFO AppClient$ClientActor: Connecting to master akka.tcp://sparkMaster@master:7077/user/Master...
16/04/14 21:07:55 INFO SparkDeploySchedulerBackend: Connected to Spark cluster with app ID app-20160414210755-0004
16/04/14 21:07:55 INFO AppClient$ClientActor: Executor added: app-20160414210755-0004/0 on worker-20160414203614-192.168.25.128-36227 (192.168.25.128:36227) with 1 cores
16/04/14 21:07:56 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160414210755-0004/0 on hostPort 192.168.25.128:36227 with 1 cores, 1000.0 MB RAM
16/04/14 21:07:56 INFO AppClient$ClientActor: Executor updated: app-20160414210755-0004/0 is now LOADING
16/04/14 21:07:56 INFO AppClient$ClientActor: Executor updated: app-20160414210755-0004/0 is now RUNNING
16/04/14 21:07:56 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38595.
16/04/14 21:07:56 INFO NettyBlockTransferService: Server created on 38595
16/04/14 21:07:56 INFO BlockManagerMaster: Trying to register BlockManager
16/04/14 21:07:56 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.25.128:38595 with 415.5 MB RAM, BlockManagerId(driver, 192.168.25.128, 38595)
16/04/14 21:07:56 INFO BlockManagerMaster: Registered BlockManager
16/04/14 21:07:57 INFO SparkDeploySchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
16/04/14 21:08:02 INFO SparkContext: Added JAR /home/hadoop/tools/jars/1.jar at http://192.168.25.128:33204/jars/1.jar with timestamp 1460639282147
16/04/14 21:08:05 INFO MemoryStore: ensureFreeSpace(130448) called with curMem=0, maxMem=435714785
16/04/14 21:08:05 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 127.4 KB, free 415.4 MB)
16/04/14 21:08:05 INFO MemoryStore: ensureFreeSpace(14257) called with curMem=130448, maxMem=435714785
16/04/14 21:08:05 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.9 KB, free 415.4 MB)
16/04/14 21:08:05 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.25.128:38595 (size: 13.9 KB, free: 415.5 MB)
16/04/14 21:08:05 INFO SparkContext: Created broadcast 0 from textFile at Ex0Wordcount.java:56
16/04/14 21:08:06 INFO FileInputFormat: Total input paths to process : 1
16/04/14 21:08:07 INFO SparkContext: Starting job: collect at Ex0Wordcount.java:104
16/04/14 21:08:07 INFO DAGScheduler: Registering RDD 3 (mapToPair at Ex0Wordcount.java:75)
16/04/14 21:08:07 INFO DAGScheduler: Got job 0 (collect at Ex0Wordcount.java:104) with 2 output partitions (allowLocal=false)
16/04/14 21:08:07 INFO DAGScheduler: Final stage: ResultStage 1(collect at Ex0Wordcount.java:104)
16/04/14 21:08:08 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
16/04/14 21:08:08 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
16/04/14 21:08:08 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at Ex0Wordcount.java:75), which has no missing parents
16/04/14 21:08:08 INFO MemoryStore: ensureFreeSpace(4800) called with curMem=144705, maxMem=435714785
16/04/14 21:08:08 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 4.7 KB, free 415.4 MB)
16/04/14 21:08:08 INFO MemoryStore: ensureFreeSpace(2699) called with curMem=149505, maxMem=435714785
16/04/14 21:08:08 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.6 KB, free 415.4 MB)
16/04/14 21:08:08 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.25.128:38595 (size: 2.6 KB, free: 415.5 MB)
16/04/14 21:08:08 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:874
16/04/14 21:08:08 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at Ex0Wordcount.java:75)
16/04/14 21:08:08 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/04/14 21:08:10 INFO SparkDeploySchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@192.168.25.128:35717/user/Executor#-1047647166]) with ID 0
16/04/14 21:08:11 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.25.128, PROCESS_LOCAL, 1468 bytes)
16/04/14 21:08:11 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.25.128:39924 with 522.0 MB RAM, BlockManagerId(0, 192.168.25.128, 39924)
16/04/14 21:08:27 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.25.128:39924 (size: 2.6 KB, free: 522.0 MB)
16/04/14 21:08:39 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, 192.168.25.128, PROCESS_LOCAL, 1468 bytes)
16/04/14 21:08:39 WARN TaskSetManager: Lost task 1.0 in stage 0.0 (TID 1, 192.168.25.128): java.io.IOException: org.apache.spark.SparkException: Failed to get broadcast_1_piece0 of broadcast_1
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1257)
	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:165)
	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:64)
	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:64)
	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:88)
	at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:62)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:70)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Failed to get broadcast_1_piece0 of broadcast_1
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1$$anonfun$2.apply(TorrentBroadcast.scala:138)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1$$anonfun$2.apply(TorrentBroadcast.scala:138)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply$mcVI$sp(TorrentBroadcast.scala:137)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:120)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:120)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$readBlocks(TorrentBroadcast.scala:120)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:175)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1254)
	... 12 more

16/04/14 21:08:39 INFO TaskSetManager: Starting task 1.1 in stage 0.0 (TID 2, 192.168.25.128, PROCESS_LOCAL, 1468 bytes)
16/04/14 21:08:39 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, 192.168.25.128): java.io.IOException: org.apache.spark.SparkException: Failed to get broadcast_0_piece0 of broadcast_0
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1257)
	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:165)
	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:64)
	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:64)
	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:88)
	at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)
	at org.apache.spark.rdd.HadoopRDD.getJobConf(HadoopRDD.scala:144)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:220)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:216)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:70)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:70)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Failed to get broadcast_0_piece0 of broadcast_0
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1$$anonfun$2.apply(TorrentBroadcast.scala:138)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1$$anonfun$2.apply(TorrentBroadcast.scala:138)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply$mcVI$sp(TorrentBroadcast.scala:137)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:120)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:120)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$readBlocks(TorrentBroadcast.scala:120)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:175)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1254)
	... 27 more

16/04/14 21:08:39 INFO TaskSetManager: Starting task 0.1 in stage 0.0 (TID 3, 192.168.25.128, PROCESS_LOCAL, 1468 bytes)
16/04/14 21:08:39 INFO TaskSetManager: Lost task 1.1 in stage 0.0 (TID 2) on executor 192.168.25.128: java.io.IOException (org.apache.spark.SparkException: Failed to get broadcast_1_piece0 of broadcast_1) [duplicate 1]
16/04/14 21:08:39 INFO TaskSetManager: Starting task 1.2 in stage 0.0 (TID 4, 192.168.25.128, PROCESS_LOCAL, 1468 bytes)
16/04/14 21:08:39 INFO TaskSetManager: Lost task 0.1 in stage 0.0 (TID 3) on executor 192.168.25.128: java.io.IOException (org.apache.spark.SparkException: Failed to get broadcast_1_piece0 of broadcast_1) [duplicate 2]
16/04/14 21:08:39 INFO TaskSetManager: Starting task 0.2 in stage 0.0 (TID 5, 192.168.25.128, PROCESS_LOCAL, 1468 bytes)
16/04/14 21:08:39 INFO TaskSetManager: Lost task 1.2 in stage 0.0 (TID 4) on executor 192.168.25.128: java.io.IOException (org.apache.spark.SparkException: Failed to get broadcast_1_piece0 of broadcast_1) [duplicate 3]
16/04/14 21:08:39 INFO TaskSetManager: Starting task 1.3 in stage 0.0 (TID 6, 192.168.25.128, PROCESS_LOCAL, 1468 bytes)
16/04/14 21:08:39 INFO TaskSetManager: Lost task 0.2 in stage 0.0 (TID 5) on executor 192.168.25.128: java.io.IOException (org.apache.spark.SparkException: Failed to get broadcast_1_piece0 of broadcast_1) [duplicate 4]
16/04/14 21:08:39 INFO TaskSetManager: Starting task 0.3 in stage 0.0 (TID 7, 192.168.25.128, PROCESS_LOCAL, 1468 bytes)
16/04/14 21:08:39 INFO TaskSetManager: Lost task 1.3 in stage 0.0 (TID 6) on executor 192.168.25.128: java.io.IOException (org.apache.spark.SparkException: Failed to get broadcast_1_piece0 of broadcast_1) [duplicate 5]
16/04/14 21:08:39 ERROR TaskSetManager: Task 1 in stage 0.0 failed 4 times; aborting job
16/04/14 21:08:39 INFO TaskSetManager: Lost task 0.3 in stage 0.0 (TID 7) on executor 192.168.25.128: java.io.IOException (org.apache.spark.SparkException: Failed to get broadcast_1_piece0 of broadcast_1) [duplicate 6]
16/04/14 21:08:39 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/04/14 21:08:39 INFO TaskSchedulerImpl: Cancelling stage 0
16/04/14 21:08:39 INFO DAGScheduler: ShuffleMapStage 0 (mapToPair at Ex0Wordcount.java:75) failed in 30.828 s
16/04/14 21:08:39 INFO DAGScheduler: Job 0 failed: collect at Ex0Wordcount.java:104, took 32.310673 s
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 6, 192.168.25.128): java.io.IOException: org.apache.spark.SparkException: Failed to get broadcast_1_piece0 of broadcast_1
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1257)
	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:165)
	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:64)
	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:64)
	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:88)
	at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:62)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:70)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Failed to get broadcast_1_piece0 of broadcast_1
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1$$anonfun$2.apply(TorrentBroadcast.scala:138)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1$$anonfun$2.apply(TorrentBroadcast.scala:138)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply$mcVI$sp(TorrentBroadcast.scala:137)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:120)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:120)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$readBlocks(TorrentBroadcast.scala:120)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:175)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1254)
	... 12 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1266)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1257)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1256)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1256)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1411)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
16/04/14 21:08:40 INFO SparkContext: Invoking stop() from shutdown hook
16/04/14 21:08:40 INFO SparkUI: Stopped Spark web UI at http://192.168.25.128:4040
16/04/14 21:08:40 INFO DAGScheduler: Stopping DAGScheduler
16/04/14 21:08:40 INFO SparkDeploySchedulerBackend: Shutting down all executors
16/04/14 21:08:40 INFO SparkDeploySchedulerBackend: Asking each executor to shut down
16/04/14 21:08:40 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/04/14 21:08:40 INFO Utils: path = /tmp/spark-f3c3dac6-604d-430e-8cf9-266f0563f7cb/blockmgr-de143055-81e3-4cc8-b71c-f8d7a35046b6, already present as root for deletion.
16/04/14 21:08:40 INFO MemoryStore: MemoryStore cleared
16/04/14 21:08:40 INFO BlockManager: BlockManager stopped
16/04/14 21:08:40 INFO BlockManagerMaster: BlockManagerMaster stopped
16/04/14 21:08:40 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/04/14 21:08:41 INFO SparkContext: Successfully stopped SparkContext
16/04/14 21:08:41 INFO Utils: Shutdown hook called
16/04/14 21:08:41 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
16/04/14 21:08:41 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
16/04/14 21:08:41 INFO Utils: Deleting directory /tmp/spark-f3c3dac6-604d-430e-8cf9-266f0563f7cb
